import streamlit as st
import pandas as pd
import boto3
import requests
from PIL import Image
from io import BytesIO
import zipfile
import re
import traceback

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Streamlit Config
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="Image â†’ ZIP + Server (DEBUG)",
    page_icon="ğŸªµ",
    layout="centered",
)

st.title("ğŸªµ Image Downloader â†’ Company Server (DEBUG MODE)")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# S3 CONFIG (FROM SECRETS)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.write("ğŸ” Loading secrets...")

try:
    AWS_ACCESS_KEY = st.secrets["AWS_ACCESS_KEY_ID"]
    AWS_SECRET_KEY = st.secrets["AWS_SECRET_ACCESS_KEY"]
    S3_BUCKET = st.secrets["S3_BUCKET"]

    S3_PREFIX = "streamlit/"
    PUBLIC_BASE_URL = "https://static.ora.ma/streamlit/"
    AWS_REGION = "eu-west-3"

    st.success("âœ… Secrets loaded successfully")

except Exception as e:
    st.error("âŒ Missing AWS / S3 secrets")
    st.exception(e)
    st.stop()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# S3 CLIENT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.write("ğŸ”Œ Creating S3 client...")

try:
    s3 = boto3.client(
        "s3",
        aws_access_key_id=AWS_ACCESS_KEY,
        aws_secret_access_key=AWS_SECRET_KEY,
        region_name=AWS_REGION,
    )
    st.success("âœ… S3 client created")

except Exception as e:
    st.error("âŒ Failed to create S3 client")
    st.exception(e)
    st.stop()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HELPERS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def sanitize_filename(name: str) -> str:
    return re.sub(r"[^A-Za-z0-9_-]", "_", name).strip() or "image"

BROWSER_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept": "image/avif,image/webp,image/apng,image/*,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9",
    "Referer": "https://glovoapp.com/",
}

st.write("ğŸŒ Browser headers:", BROWSER_HEADERS)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FILE UPLOAD
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
uploaded = st.file_uploader("ğŸ“¤ Upload CSV or XLSX", type=["csv", "xlsx"])

if uploaded:
    st.write("ğŸ“„ File uploaded:", uploaded.name)

    try:
        if uploaded.name.lower().endswith(".csv"):
            df = pd.read_csv(uploaded)
        else:
            df = pd.read_excel(uploaded)

        st.success("âœ… File loaded into DataFrame")

    except Exception as e:
        st.error("âŒ Failed to read file")
        st.exception(e)
        st.stop()

    st.subheader("ğŸ“Œ Columns detected")
    st.write(df.columns.tolist())

    product_col = st.selectbox("Select product column", df.columns)
    url_col = st.selectbox("Select image URL column", df.columns)

    if st.button("ğŸš€ Process Images (DEBUG)"):

        st.write("â–¶ï¸ Starting processing...")
        zip_buffer = BytesIO()
        server_urls = [None] * len(df)

        uploaded_count = 0
        skipped_count = 0

        with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zipf:

            for idx, row in df.iterrows():
                st.write(f"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
                st.write(f"ğŸ” ROW {idx}")

                try:
                    product = str(row[product_col]).strip()
                    url = str(row[url_col]).strip()

                    st.write("ğŸ“ Product:", product)
                    st.write("ğŸ”— URL:", url)

                    if not url.startswith("http"):
                        st.warning("âš ï¸ Invalid URL â†’ skipped")
                        skipped_count += 1
                        continue

                    filename = sanitize_filename(product) + ".jpg"
                    s3_key = S3_PREFIX + filename

                    st.write("ğŸ§¾ Filename:", filename)
                    st.write("ğŸ“‚ S3 Key:", s3_key)

                    # â”€â”€â”€â”€â”€ DOWNLOAD IMAGE â”€â”€â”€â”€â”€
                    st.write("â¬‡ï¸ Downloading image...")
                    r = requests.get(url, headers=BROWSER_HEADERS, timeout=25)

                    st.write("ğŸŒ HTTP status:", r.status_code)
                    st.write("ğŸ“¦ Content-Type:", r.headers.get("Content-Type"))

                    r.raise_for_status()

                    img = Image.open(BytesIO(r.content))
                    st.write("ğŸ–¼ Image format:", img.format, "| mode:", img.mode)

                    if img.mode == "RGBA":
                        img = img.convert("RGB")
                        st.write("ğŸ¨ Converted RGBA â†’ RGB")

                    # â”€â”€â”€â”€â”€ SAVE JPG â”€â”€â”€â”€â”€
                    img_bytes = BytesIO()
                    img.save(img_bytes, "JPEG", quality=90)
                    img_bytes.seek(0)

                    st.write("ğŸ’¾ Image converted to JPG in memory")

                    # â”€â”€â”€â”€â”€ UPLOAD TO S3 â”€â”€â”€â”€â”€
                    st.write("â˜ï¸ Uploading to S3...")

                    s3.upload_fileobj(
                        img_bytes,
                        S3_BUCKET,
                        s3_key,
                        ExtraArgs={
                            "ContentType": "image/jpeg",
                            "ACL": "public-read",
                        },
                    )

                    public_url = PUBLIC_BASE_URL + filename
                    server_urls[idx] = public_url

                    st.success(f"âœ… Uploaded to: {public_url}")

                    # â”€â”€â”€â”€â”€ ADD TO ZIP â”€â”€â”€â”€â”€
                    zipf.writestr(filename, img_bytes.getvalue())
                    st.write("ğŸ“¦ Added to ZIP")

                    uploaded_count += 1

                except Exception as e:
                    skipped_count += 1
                    server_urls[idx] = None

                    st.error("âŒ ERROR on this row")
                    st.text(traceback.format_exc())

        df["Server Image URL"] = server_urls

        st.success(f"ğŸ‰ DONE | Uploaded: {uploaded_count} | Skipped: {skipped_count}")

        st.download_button(
            "â¬‡ï¸ Download Images ZIP",
            data=zip_buffer.getvalue(),
            file_name="images.zip",
            mime="application/zip",
        )

        st.download_button(
            "â¬‡ï¸ Download Updated CSV",
            data=df.to_csv(index=False).encode("utf-8"),
            file_name="updated_with_server_links.csv",
            mime="text/csv",
        )
